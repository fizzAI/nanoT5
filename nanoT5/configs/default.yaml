defaults:
    - _self_
    - task: pt
    - local_env: default

# Experiment args
mode: 'pt'
device: gpu
precision: 'bf16'
eval_only: false
predict_only: false
seed: 2137

model:
    klass: local_t5
    name: 'google/t5-v1_1-base'
    overwrite: # overwrite config with these values
        dropout_rate: 0.0
    add_config: # add these values to the config
        is_bf16: false
    checkpoint_path: ''
    random_init: true
    compile: true # Pytorch 2.0

data:
    input_length: 512
    mlm_probability: 0.15
    mean_noise_span_length: 3.0
    num_workers: 8

optim:
    name: precond_schedule_palm_foreach_soap # PrecondSchedulePaLMForeachSOAP
    base_lr: 1e-3
    batch_size: 256
    total_steps: 65536
    epochs: -1 # If it's > 0 it overwrites total_steps
    warmup_steps: 10000
    lr_scheduler: cosine
    weight_decay: 0.01
    grad_clip: 1.0
    grad_acc: 2
    final_cosine: 1e-5

eval:
    every_steps: 1000 # Eval once in the end
    steps: 500

checkpoint:
    every_steps: 5000 # Save checkpoint once in the end

logging:
    every_steps: 5
    grad_l2: true
    weights_l2: true
    use_wandb: true
    # Can remove or comment out the below if not using Weights & Biases
    wandb_config:
        project: nanoT5
        entity: 'fizzzz'
        tags: ['nanoT5']
        mode: 'online'

hydra:
    job:
        chdir: True
